{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise on the Quantum Classifier**"
      ],
      "metadata": {
        "id": "fzjm6C_L1Dtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uODnkh-0Uh5t",
        "outputId": "8e78572b-48d0-43e0-c55a-4d230b5dddcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.39.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.4.2)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Collecting pennylane-lightning>=0.39 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.8.30)\n",
            "Downloading PennyLane-0.39.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: appdirs, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.0 pennylane-0.39.0 pennylane-lightning-0.39.0 rustworkx-0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example I: From Pennylane Demos > Fitting the parity function**\n",
        "\n",
        "For this practice please make sure to download both parity_train.txt and parity_test.txt data from the following links:\n",
        "\n",
        "https://drive.google.com/file/d/13ImDzaJpq1KhM9K4Ian3vfdczXR18EJg/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/1k3nTcsReZePZ0S19gXcsI8KKapFW0wyA/view?usp=sharing"
      ],
      "metadata": {
        "id": "8vRg9M-mZIFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.optimize import NesterovMomentumOptimizer\n",
        "\n",
        "# Quantum and classical nodes\n",
        "\n",
        "dev = qml.device(\"default.qubit\")\n",
        "\n",
        "def layer(layer_weights):\n",
        "    for wire in range(4):\n",
        "        qml.Rot(*layer_weights[wire], wires=wire)\n",
        "\n",
        "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
        "        qml.CNOT(wires)\n",
        "def state_preparation(x):\n",
        "    qml.BasisState(x, wires=[0, 1, 2, 3])\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, x):\n",
        "    state_preparation(x)\n",
        "\n",
        "    for layer_weights in weights:\n",
        "        layer(layer_weights)\n",
        "\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "def variational_classifier(weights, bias, x):\n",
        "    return circuit(weights, x) + bias\n",
        "\n",
        "\n",
        "# Cost\n",
        "\n",
        "def square_loss(labels, predictions):\n",
        "    # We use a call to qml.math.stack to allow subtracting the arrays directly\n",
        "    return np.mean((labels - qml.math.stack(predictions)) ** 2)\n",
        "def accuracy(labels, predictions):\n",
        "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
        "    acc = acc / len(labels)\n",
        "    return acc\n",
        "def cost(weights, bias, X, Y):\n",
        "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
        "    return square_loss(Y, predictions)\n",
        "\n",
        "\n",
        "# Optimization\n",
        "\n",
        "data = np.loadtxt(\"parity_train.txt\", dtype=int)\n",
        "X = np.array(data[:, :-1])\n",
        "Y = np.array(data[:, -1])\n",
        "Y = Y * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
        "\n",
        "for x,y in zip(X, Y):\n",
        "    print(f\"x = {x}, y = {y}\")\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "num_qubits = 4\n",
        "num_layers = 2\n",
        "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
        "bias_init = np.array(0.0, requires_grad=True)\n",
        "\n",
        "print(\"Weights:\", weights_init)\n",
        "print(\"Bias: \", bias_init)\n",
        "\n",
        "\n",
        "opt = NesterovMomentumOptimizer(0.5)\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "weights = weights_init\n",
        "bias = bias_init\n",
        "for it in range(100):\n",
        "\n",
        "    # Update the weights by one optimizer step, using only a limited batch of data\n",
        "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
        "    X_batch = X[batch_index]\n",
        "    Y_batch = Y[batch_index]\n",
        "    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n",
        "\n",
        "    # Compute accuracy\n",
        "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
        "\n",
        "    current_cost = cost(weights, bias, X, Y)\n",
        "    acc = accuracy(Y, predictions)\n",
        "\n",
        "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")\n",
        "\n",
        "\n",
        "data = np.loadtxt(\"parity_test.txt\", dtype=int)\n",
        "X_test = np.array(data[:, :-1])\n",
        "Y_test = np.array(data[:, -1])\n",
        "Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
        "\n",
        "predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
        "\n",
        "for x,y,p in zip(X_test, Y_test, predictions_test):\n",
        "    print(f\"x = {x}, y = {y}, pred={p}\")\n",
        "\n",
        "acc_test = accuracy(Y_test, predictions_test)\n",
        "print(\"Accuracy on unseen data:\", acc_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLdPhBQTXfTm",
        "outputId": "f76731f3-9d44-49b6-c1ed-9c34f08f59c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = [0 0 0 1], y = 1\n",
            "x = [0 0 1 0], y = 1\n",
            "x = [0 1 0 0], y = 1\n",
            "x = [0 1 0 1], y = -1\n",
            "x = [0 1 1 0], y = -1\n",
            "x = [0 1 1 1], y = 1\n",
            "x = [1 0 0 0], y = 1\n",
            "x = [1 0 0 1], y = -1\n",
            "x = [1 0 1 1], y = 1\n",
            "x = [1 1 1 1], y = -1\n",
            "Weights: [[[ 0.01764052  0.00400157  0.00978738]\n",
            "  [ 0.02240893  0.01867558 -0.00977278]\n",
            "  [ 0.00950088 -0.00151357 -0.00103219]\n",
            "  [ 0.00410599  0.00144044  0.01454274]]\n",
            "\n",
            " [[ 0.00761038  0.00121675  0.00443863]\n",
            "  [ 0.00333674  0.01494079 -0.00205158]\n",
            "  [ 0.00313068 -0.00854096 -0.0255299 ]\n",
            "  [ 0.00653619  0.00864436 -0.00742165]]]\n",
            "Bias:  0.0\n",
            "Iter:    1 | Cost: 2.3147651 | Accuracy: 0.5000000\n",
            "Iter:    2 | Cost: 1.9664866 | Accuracy: 0.5000000\n",
            "Iter:    3 | Cost: 1.9208589 | Accuracy: 0.5000000\n",
            "Iter:    4 | Cost: 2.6276126 | Accuracy: 0.5000000\n",
            "Iter:    5 | Cost: 0.9323119 | Accuracy: 0.6000000\n",
            "Iter:    6 | Cost: 1.1903549 | Accuracy: 0.5000000\n",
            "Iter:    7 | Cost: 2.0508989 | Accuracy: 0.4000000\n",
            "Iter:    8 | Cost: 1.1275531 | Accuracy: 0.6000000\n",
            "Iter:    9 | Cost: 1.1659803 | Accuracy: 0.6000000\n",
            "Iter:   10 | Cost: 1.1349618 | Accuracy: 0.6000000\n",
            "Iter:   11 | Cost: 0.9994063 | Accuracy: 0.6000000\n",
            "Iter:   12 | Cost: 1.0812559 | Accuracy: 0.6000000\n",
            "Iter:   13 | Cost: 1.2863155 | Accuracy: 0.6000000\n",
            "Iter:   14 | Cost: 2.2658259 | Accuracy: 0.4000000\n",
            "Iter:   15 | Cost: 1.1323724 | Accuracy: 0.6000000\n",
            "Iter:   16 | Cost: 1.3439737 | Accuracy: 0.8000000\n",
            "Iter:   17 | Cost: 2.0076168 | Accuracy: 0.6000000\n",
            "Iter:   18 | Cost: 1.2685760 | Accuracy: 0.5000000\n",
            "Iter:   19 | Cost: 1.6762475 | Accuracy: 0.5000000\n",
            "Iter:   20 | Cost: 1.1868237 | Accuracy: 0.6000000\n",
            "Iter:   21 | Cost: 1.4784687 | Accuracy: 0.6000000\n",
            "Iter:   22 | Cost: 1.4599473 | Accuracy: 0.6000000\n",
            "Iter:   23 | Cost: 0.9573269 | Accuracy: 0.6000000\n",
            "Iter:   24 | Cost: 1.1657424 | Accuracy: 0.5000000\n",
            "Iter:   25 | Cost: 1.0877087 | Accuracy: 0.4000000\n",
            "Iter:   26 | Cost: 1.1683687 | Accuracy: 0.6000000\n",
            "Iter:   27 | Cost: 2.1141689 | Accuracy: 0.6000000\n",
            "Iter:   28 | Cost: 1.0272966 | Accuracy: 0.5000000\n",
            "Iter:   29 | Cost: 0.9664085 | Accuracy: 0.5000000\n",
            "Iter:   30 | Cost: 1.1287654 | Accuracy: 0.6000000\n",
            "Iter:   31 | Cost: 1.4202360 | Accuracy: 0.4000000\n",
            "Iter:   32 | Cost: 1.1286000 | Accuracy: 0.5000000\n",
            "Iter:   33 | Cost: 1.9594333 | Accuracy: 0.4000000\n",
            "Iter:   34 | Cost: 1.2811832 | Accuracy: 0.4000000\n",
            "Iter:   35 | Cost: 0.8522775 | Accuracy: 0.7000000\n",
            "Iter:   36 | Cost: 1.4765281 | Accuracy: 0.6000000\n",
            "Iter:   37 | Cost: 0.9603287 | Accuracy: 0.6000000\n",
            "Iter:   38 | Cost: 1.6031314 | Accuracy: 0.6000000\n",
            "Iter:   39 | Cost: 1.1700888 | Accuracy: 0.4000000\n",
            "Iter:   40 | Cost: 1.7571779 | Accuracy: 0.4000000\n",
            "Iter:   41 | Cost: 1.9608116 | Accuracy: 0.6000000\n",
            "Iter:   42 | Cost: 2.0802752 | Accuracy: 0.6000000\n",
            "Iter:   43 | Cost: 1.1904884 | Accuracy: 0.3000000\n",
            "Iter:   44 | Cost: 0.9941585 | Accuracy: 0.6000000\n",
            "Iter:   45 | Cost: 1.0709609 | Accuracy: 0.5000000\n",
            "Iter:   46 | Cost: 0.9780625 | Accuracy: 0.6000000\n",
            "Iter:   47 | Cost: 1.1573709 | Accuracy: 0.6000000\n",
            "Iter:   48 | Cost: 1.0235239 | Accuracy: 0.6000000\n",
            "Iter:   49 | Cost: 1.2842469 | Accuracy: 0.5000000\n",
            "Iter:   50 | Cost: 0.8549226 | Accuracy: 0.6000000\n",
            "Iter:   51 | Cost: 0.5136787 | Accuracy: 1.0000000\n",
            "Iter:   52 | Cost: 0.2488031 | Accuracy: 1.0000000\n",
            "Iter:   53 | Cost: 0.0461277 | Accuracy: 1.0000000\n",
            "Iter:   54 | Cost: 0.0293518 | Accuracy: 1.0000000\n",
            "Iter:   55 | Cost: 0.0205454 | Accuracy: 1.0000000\n",
            "Iter:   56 | Cost: 0.0352514 | Accuracy: 1.0000000\n",
            "Iter:   57 | Cost: 0.0576767 | Accuracy: 1.0000000\n",
            "Iter:   58 | Cost: 0.0291305 | Accuracy: 1.0000000\n",
            "Iter:   59 | Cost: 0.0127137 | Accuracy: 1.0000000\n",
            "Iter:   60 | Cost: 0.0058108 | Accuracy: 1.0000000\n",
            "Iter:   61 | Cost: 0.0018002 | Accuracy: 1.0000000\n",
            "Iter:   62 | Cost: 0.0014089 | Accuracy: 1.0000000\n",
            "Iter:   63 | Cost: 0.0017489 | Accuracy: 1.0000000\n",
            "Iter:   64 | Cost: 0.0021282 | Accuracy: 1.0000000\n",
            "Iter:   65 | Cost: 0.0029876 | Accuracy: 1.0000000\n",
            "Iter:   66 | Cost: 0.0035331 | Accuracy: 1.0000000\n",
            "Iter:   67 | Cost: 0.0035540 | Accuracy: 1.0000000\n",
            "Iter:   68 | Cost: 0.0025639 | Accuracy: 1.0000000\n",
            "Iter:   69 | Cost: 0.0019459 | Accuracy: 1.0000000\n",
            "Iter:   70 | Cost: 0.0015856 | Accuracy: 1.0000000\n",
            "Iter:   71 | Cost: 0.0008439 | Accuracy: 1.0000000\n",
            "Iter:   72 | Cost: 0.0005960 | Accuracy: 1.0000000\n",
            "Iter:   73 | Cost: 0.0003122 | Accuracy: 1.0000000\n",
            "Iter:   74 | Cost: 0.0002446 | Accuracy: 1.0000000\n",
            "Iter:   75 | Cost: 0.0001745 | Accuracy: 1.0000000\n",
            "Iter:   76 | Cost: 0.0001215 | Accuracy: 1.0000000\n",
            "Iter:   77 | Cost: 0.0001141 | Accuracy: 1.0000000\n",
            "Iter:   78 | Cost: 0.0001538 | Accuracy: 1.0000000\n",
            "Iter:   79 | Cost: 0.0001871 | Accuracy: 1.0000000\n",
            "Iter:   80 | Cost: 0.0001330 | Accuracy: 1.0000000\n",
            "Iter:   81 | Cost: 0.0001380 | Accuracy: 1.0000000\n",
            "Iter:   82 | Cost: 0.0001336 | Accuracy: 1.0000000\n",
            "Iter:   83 | Cost: 0.0001483 | Accuracy: 1.0000000\n",
            "Iter:   84 | Cost: 0.0001234 | Accuracy: 1.0000000\n",
            "Iter:   85 | Cost: 0.0001359 | Accuracy: 1.0000000\n",
            "Iter:   86 | Cost: 0.0001268 | Accuracy: 1.0000000\n",
            "Iter:   87 | Cost: 0.0002270 | Accuracy: 1.0000000\n",
            "Iter:   88 | Cost: 0.0000865 | Accuracy: 1.0000000\n",
            "Iter:   89 | Cost: 0.0000774 | Accuracy: 1.0000000\n",
            "Iter:   90 | Cost: 0.0000759 | Accuracy: 1.0000000\n",
            "Iter:   91 | Cost: 0.0000607 | Accuracy: 1.0000000\n",
            "Iter:   92 | Cost: 0.0000523 | Accuracy: 1.0000000\n",
            "Iter:   93 | Cost: 0.0000536 | Accuracy: 1.0000000\n",
            "Iter:   94 | Cost: 0.0000444 | Accuracy: 1.0000000\n",
            "Iter:   95 | Cost: 0.0000384 | Accuracy: 1.0000000\n",
            "Iter:   96 | Cost: 0.0000497 | Accuracy: 1.0000000\n",
            "Iter:   97 | Cost: 0.0000263 | Accuracy: 1.0000000\n",
            "Iter:   98 | Cost: 0.0000229 | Accuracy: 1.0000000\n",
            "Iter:   99 | Cost: 0.0000339 | Accuracy: 1.0000000\n",
            "Iter:  100 | Cost: 0.0000174 | Accuracy: 1.0000000\n",
            "x = [0 0 0 0], y = -1, pred=-1.0\n",
            "x = [0 0 1 1], y = -1, pred=-1.0\n",
            "x = [1 0 1 0], y = -1, pred=-1.0\n",
            "x = [1 1 1 0], y = 1, pred=1.0\n",
            "x = [1 1 0 0], y = -1, pred=-1.0\n",
            "x = [1 1 0 1], y = 1, pred=1.0\n",
            "Accuracy on unseen data: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part II: Practice**"
      ],
      "metadata": {
        "id": "MGCXKNohaDuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Marketing**\n",
        "\n",
        " Advertising and Marketing\n",
        "Customers may have various consumption profiles based on, for example, their age, frequency of purchases, intensity of their web store browsing (in minutes/day) and the budget spent on monthly purchases (in euros). This data can be used to determine whether a customer is likely to buy (or not) more items and thus send them (or not) promotional offers and discounts. Build the circuit that allows the mapping of a customer's consumption characteristics into quantum data to then decide whether or not to propose more advertising to a customer. Use for this the gate σ_{3} and a circuit depth Z equal to 2. You have to create the circuit using “IBM Quantum Composer” and deliver the OPENQASM code generated by the composer. Also, as a second option, you can use QISKIT to build the circuit. In both options, the deliverable would be a link to your Google Colab notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "NYqLVJ8QpRcV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0HuHXBX0eeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Finance**\n",
        "\n",
        "After the mapping of some economic data used to classify economies with finance at risk or not, we admit that the quantum data obtained after the mapping of quantum data produce the state |ψ> = α |0000> + β|1001> + ᵞ |0101> + σ|1111>. Build the circuit that corresponds to the classifier that allows the classification of this state with a depth equal to 2. You have to create the circuit using “IBM Quantum Composer” and deliver the OPENQASM code generated by the composer. Also, as a second option, you can use QISKIT to build the circuit. In both options, the deliverable would be a link to your Google Colab notebook."
      ],
      "metadata": {
        "id": "TaB5aZzsprYi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UkX5VROM0fQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3: Earthquake Classifier**\n",
        "\n",
        "Natural phenomena can be catastrophic. It is therefore important to develop prevention systems to classify high-risk areas. As an example, several types of earthquakes exist: volcanic, human, etc. Regardless of their type, let us assume that an earthquake depends on the frequency of tectonic plate events, as well as volcanic activity and mining facilities. We want to develop a quantum variational classifier that takes these characteristics as input to know whether an area is at risk of earthquakes or not.\n",
        "1. Construct the included quantum variational classifier, U_{Φ(X_{i})} and U_{Φ(W(Φ)} using the transformation σ_{1} and a circuit depth Z equal to 2 for the transformation U_{Φ(X_{i})} and a depth of 1 for U_{Φ(W(Φ)}?\n",
        "2.   Run the circuit with an IBM simulator, indicating the output of the classifier C computes Ŷ when the input data are 40, 3, and 10 and Θ = Π/3? The output refers to class 0 (earthquake is unlikely to occur) or 1 (earthquake is unlikely to occur).\n",
        "You have to create the circuit using “IBM Quantum Composer” and deliver the OPENQASM code generated by the composer. Also, as a second option, you can use QISKIT to build the circuit. In both options, the deliverable would be a link to your Google Colab notebook.\n"
      ],
      "metadata": {
        "id": "R5w1pVFMqlPS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VEjV8B30f8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}